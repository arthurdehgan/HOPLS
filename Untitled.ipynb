{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using numpy backend.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import tucker\n",
    "from tensorly.decomposition._tucker import partial_tucker\n",
    "from tensorly.tenalg import mode_dot, multi_mode_dot, kronecker\n",
    "import numpy as np\n",
    "from numpy.linalg import svd, pinv\n",
    "\n",
    "\n",
    "def rmsep(y_true, y_pred):\n",
    "    \"\"\"Compute Root Mean Square Error between two arrays.\"\"\"\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2, axis=0))\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    \"\"\"Compute Root Mean Square Percentage Error between two arrays.\"\"\"\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2, axis=0))\n",
    "\n",
    "\n",
    "def qsquared(y_true, y_pred):\n",
    "    \"\"\"Compute the Q^2 Error between two arrays.\"\"\"\n",
    "    return 1 - ((tl.norm(y_true - y_pred) ** 2) / (tl.norm(y_true) ** 2))\n",
    "\n",
    "\n",
    "def cov(A, B):\n",
    "    \"\"\"Computes the mode 1 (mode 0 in python) contraction of 2 matrices.\"\"\"\n",
    "    assert A.shape[0] == B.shape[0], \"A and B need to have the same shape on axis 0\"\n",
    "    dimension_A = A.shape[1:]\n",
    "    dimension_B = B.shape[1:]\n",
    "    dimensions = list(dimension_A) + list(dimension_B)\n",
    "    rmode_A = len(dimension_A)\n",
    "    dim = A.shape[0]\n",
    "    C = tl.zeros(dimensions)\n",
    "    indices = []\n",
    "    for mode in dimensions:\n",
    "        indices.append(range(mode))\n",
    "    for idx in product(*indices):\n",
    "        idx_A, idx_B = list(idx[:rmode_A]), list(idx[rmode_A:])\n",
    "        C[idx] = np.sum(\n",
    "            [A[tuple([i] + idx_A)] * B[tuple([i] + idx_B)] for i in range(dim)]\n",
    "        )\n",
    "    return C\n",
    "    # alpha = \"bcdefghijklmnopqrstuvwxyz\"\n",
    "    # A_alpha = alpha[: len(A.shape) - 1]\n",
    "    # B_alpha = alpha[1 - len(B.shape) :]\n",
    "    # string = \"a\" + A_alpha + \",\" + B_alpha + \"->\" + A_alpha + B_alpha\n",
    "    # print(A.shape, B.shape)\n",
    "    # return np.einsum(\"a...,a...->...\", A, B)\n",
    "    # return\n",
    "\n",
    "\n",
    "class HOPLS:\n",
    "    def __init__(self, R, Ln, Kn=None, metric=None, epsilon=0):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            R: int, The number of latent vectors.\n",
    "\n",
    "            Ln: list, the ranks for the decomposition of X: [L2, ..., LN].\n",
    "\n",
    "            Kn: list, the ranks for the decomposition of Y: [K2, ..., KM].\n",
    "\n",
    "            epsilon: Float, default: 10e-7, The implicit secondary criterion of the\n",
    "                algorithm. The algorithm will stop if we have not reached R but the\n",
    "                residuals have a norm smaller than epsilon.\n",
    "        \"\"\"\n",
    "        self.R = R\n",
    "        self.Ln = Ln\n",
    "        self.Kn = Kn\n",
    "        self.epsilon = epsilon\n",
    "        if metric is None:\n",
    "            self.metric = qsquared\n",
    "        else:\n",
    "            self.metric = metric\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Compute the HOPLS for X and Y wrt the parameters R, Ln and Kn.\n",
    "\n",
    "        Parameters:\n",
    "            X: tensorly Tensor, The target tensor of shape [i1, ... iN], N >= 3.\n",
    "\n",
    "            Y: tensorly Tensor, The target tensor of shape [j1, ... jM], M >= 3.\n",
    "\n",
    "        Returns:\n",
    "            G: Tensor, The core Tensor of the HOPLS for X, of shape (R, L2, ..., LN).\n",
    "\n",
    "            P: List, The N-1 loadings of X. of shape (R, I(n+1), L(n+1)) for n from 1 to N-1.\n",
    "\n",
    "            D: Tensor, The core Tensor of the HOPLS for Y, of shape (R, K2, ..., KN).\n",
    "\n",
    "            Q: List, The N-1 loadings of Y.\n",
    "\n",
    "            ts: Tensor, The latent vectors of the HOPLS, of shape (i1, R).\n",
    "        \"\"\"\n",
    "        # check parameters\n",
    "        X_mode = len(X.shape)\n",
    "        Y_mode = len(Y.shape)\n",
    "        assert Y_mode >= 2, \"Y need to be mode 2 minimum.\"\n",
    "        assert X_mode >= 3, \"X need to be mode 3 minimum.\"\n",
    "        assert (\n",
    "            len(self.Ln) == X_mode - 1\n",
    "        ), f\"The list of ranks for the decomposition of X (Ln) need to be equal to the mode of X -1: {X_mode-1}.\"\n",
    "        if Y_mode == 2:\n",
    "            return self._fit_2d(X, Y)\n",
    "        assert (\n",
    "            len(self.Kn) == Y_mode - 1\n",
    "        ), f\"The list of ranks for the decomposition of Y (Kn) need to be equal to the mode of Y -1: {Y_mode-1}.\"\n",
    "        # Initialization\n",
    "        Er, Fr = X, Y\n",
    "        In = X.shape\n",
    "        P, Q = [], []\n",
    "        G = tl.zeros((self.R, *self.Ln))\n",
    "        D = tl.zeros((self.R, *self.Kn))\n",
    "        T = tl.zeros((In[0], self.R))\n",
    "        # Beginning of the algorithm\n",
    "        for r in range(self.R):\n",
    "            if tl.norm(Er) > self.epsilon and tl.norm(Fr) > self.epsilon:\n",
    "                Cr = cov(Er, Fr)\n",
    "                # HOOI tucker decomposition of C\n",
    "                _, latents = tucker(\n",
    "                    Cr,ranks=self.Ln + self.Kn, n_iter_max=int(1e6), tol=1e-7\n",
    "                )\n",
    "                print(np.shape(latents))\n",
    "                # Getting P and Q\n",
    "                Pr = latents[: len(Er.shape) - 1]\n",
    "                Qr = latents[len(Er.shape) - 1 :]\n",
    "                # computing product of Er by latents of X\n",
    "                tr = multi_mode_dot(Er, Pr, list(range(1, len(Pr))), transpose=True)\n",
    "                # Getting t as the first leading left singular vector of the product\n",
    "                tr = svd(tl.unfold(tr, 0))[0][:, 0]\n",
    "                tr = tr[..., np.newaxis]\n",
    "                # recomposition of the core tensors\n",
    "                Gr = tl.tucker_to_tensor(Er, [tr] + Pr, transpose_factors=True)\n",
    "                Dr = tl.tucker_to_tensor(Fr, [tr] + Qr, transpose_factors=True)\n",
    "                # Deflation\n",
    "                Er = Er - tl.tucker_to_tensor(Gr, [tr] + Pr)\n",
    "                Fr = Fr - tl.tucker_to_tensor(Dr, [tr] + Qr)\n",
    "                # Gathering of\n",
    "                P.append(Pr)\n",
    "                Q.append(Qr)\n",
    "                G[r] = Gr[0]\n",
    "                D[r] = Dr[0]\n",
    "                T[:, r] = tr[:, 0]\n",
    "            else:\n",
    "                break\n",
    "        self.model = (P, Q, G, D, T)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, Y):\n",
    "        \"\"\"Compute the HOPLS for X and Y wrt the parameters R, Ln and Kn.\n",
    "\n",
    "        Parameters:\n",
    "            X: tensorly Tensor, The tensor we wish to do a prediction from.\n",
    "            Of shape [i1, ... iN], N >= 3.\n",
    "\n",
    "            Y: tensorly Tensor, used only for the shape of the prediction.\n",
    "\n",
    "        Returns:\n",
    "            Y_pred: tensorly Tensor, The predicted Y from the model.\n",
    "        \"\"\"\n",
    "        P, Q, G, D, T = self.model\n",
    "        N = len(self.Ln)\n",
    "        M = len(self.Kn)\n",
    "        G_pi = []\n",
    "        if len(Y.shape) == 2:\n",
    "            Q_star = tl.zeros(Q.shape)\n",
    "        else:\n",
    "            Q_star = []\n",
    "        W = tl.zeros((*X.shape[1:], self.R)).reshape(-1, self.R)\n",
    "        for r in range(self.R):\n",
    "            G_pi.append(pinv(G[r]))\n",
    "            W[:, r] = np.matmul(\n",
    "                kronecker([P[r][N - n - 1] for n in range(N)]), G_pi[-1].reshape(-1)\n",
    "            )\n",
    "            if len(Y.shape) > 2:\n",
    "                Q_star.append(\n",
    "                    np.matmul(\n",
    "                        D[r].reshape(-1),\n",
    "                        kronecker([Q[r][M - n - 1] for n in range(M)]).T,\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                Q_star[:, r] = D[r, r] * Q[:, r]\n",
    "\n",
    "        if len(Y.shape) > 2:\n",
    "            Q_star = tl.tensor(Q_star).T\n",
    "        return np.matmul(np.matmul(tl.unfold(X, 0), W), Q_star.T).reshape(Y.shape)\n",
    "        # return np.matmul(T, Q_star.T).reshape(Y.shape)\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        self.fit(X, Y)\n",
    "        Y_pred = self.predict(X, Y)\n",
    "        return self.metric(Y.reshape(Y.shape[0], -1), Y_pred.reshape(Y.shape[0], -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "import tensorly as tl\n",
    "from tensorly.tenalg.n_mode_product import mode_dot\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from joblib import Parallel, delayed\n",
    "from hopls import HOPLS, qsquared\n",
    "\n",
    "\n",
    "def hyper_params_search(data, target, metric=None, verbose=True):\n",
    "    old_error = -np.inf\n",
    "    # Just like in the paper, we simplify by having l for all ranks\n",
    "    for R, l in product(range(3, 7), range(3, 10)):\n",
    "        hopls = HOPLS(\n",
    "            R, [l] * (len(data.shape) - 1), [l] * (len(data.shape) - 1), metric=None\n",
    "        )\n",
    "        error = np.mean(hopls.score(data, target))\n",
    "        if error > old_error:\n",
    "            old_error = error\n",
    "            best_params = [R, l, error]\n",
    "    if verbose:\n",
    "        print(\"Best model is with R={} and l={}, error={:.2f}\".format(*best_params))\n",
    "    return best_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = tl.tensor(np.random.normal(size=(10, 5)))\n",
    "P = tl.tensor(np.random.normal(size=(5, 7, 7)))\n",
    "Q = tl.tensor(np.random.normal(size=(5, 7, 7)))\n",
    "# Q = tl.tensor(np.random.normal(size=(5, 12)))\n",
    "E = tl.tensor(np.random.normal(size=(10, 10, 11)))\n",
    "F = tl.tensor(np.random.normal(size=(10, 10, 10)))\n",
    "X = mode_dot(P, T, 0)\n",
    "Y = mode_dot(Q, T, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "X_mode = len(X.shape)\n",
    "Y_mode = len(Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "Er, Fr = X, Y\n",
    "In = X.shape\n",
    "R,Ln,Kn = 5,[l] * (len(X.shape) - 1),[l] * (len(X.shape) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, Q = [], []\n",
    "G = tl.zeros((R, *Ln))\n",
    "D = tl.zeros((R, *Kn))\n",
    "T = tl.zeros((In[0], R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latents[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-287-91414e2c3d42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# HOOI tucker decomposition of C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         _, latents = tucker(\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mCr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mranks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mLn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         )\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorly/decomposition/_tucker.py\u001b[0m in \u001b[0;36mtucker\u001b[0;34m(tensor, rank, ranks, n_iter_max, init, svd, tol, random_state, verbose)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mmodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     return partial_tucker(tensor, modes, rank=rank, ranks=ranks, n_iter_max=n_iter_max, init=init,\n\u001b[0;32m--> 149\u001b[0;31m                           svd=svd, tol=tol, random_state=random_state, verbose=verbose)\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorly/decomposition/_tucker.py\u001b[0m in \u001b[0;36mpartial_tucker\u001b[0;34m(tensor, modes, rank, n_iter_max, init, tol, svd, random_state, verbose, ranks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mfactors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0meigenvecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_eigenvecs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mfactors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meigenvecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for r in range(R):\n",
    "    if tl.norm(Er) > epsilon and tl.norm(Fr) > epsilon:\n",
    "        Cr = cov(Er, Fr)\n",
    "        \n",
    "        # HOOI tucker decomposition of C\n",
    "        _, latents = tucker(\n",
    "            Cr,ranks=[1]+Ln, n_iter_max=int(1e6), tol=1e-7\n",
    "        )\n",
    "        print(np.shape(latents))\n",
    "        # Getting P and Q\n",
    "        Pr = latents[: len(Er.shape) - 1]\n",
    "        Qr = latents[len(Er.shape) - 1 :]\n",
    "        # computing product of Er by latents of X\n",
    "        tr = multi_mode_dot(Er, Pr, list(range(1, len(Pr))), transpose=True)\n",
    "        # Getting t as the first leading left singular vector of the product\n",
    "        tr = svd(tl.unfold(tr, 0))[0][:, 0]\n",
    "        tr = tr[..., np.newaxis]\n",
    "        # recomposition of the core tensors\n",
    "        Gr = tl.tucker_to_tensor(Er, [tr] + Pr, transpose_factors=True)\n",
    "        Dr = tl.tucker_to_tensor(Fr, [tr] + Qr, transpose_factors=True)\n",
    "        # Deflation\n",
    "        Er = Er - tl.tucker_to_tensor(Gr, [tr] + Pr)\n",
    "        Fr = Fr - tl.tucker_to_tensor(Dr, [tr] + Qr)\n",
    "        # Gathering of\n",
    "        P.append(Pr)\n",
    "        Q.append(Qr)\n",
    "        G[r] = Gr[0]\n",
    "        D[r] = Dr[0]\n",
    "        T[:, r] = tr[:, 0]\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 9)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialization\n",
    "\n",
    "\n",
    "# Beginning of the algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core, factors = partial_tucker(tensor, modes=[1, 2], ranks=[4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod = np.dot(factors[0],factors[0].T)\n",
    "np.shape(prod)\n",
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tl.tensor(np.random.normal(size=(10, 10, 11,13)))*10\n",
    "core, latents = tl.decomposition.tucker(X,rank=11)\n",
    "X_recon = tl.tenalg.multi_mode_dot(core, latents, modes=[0,1,2,3],transpose=False)\n",
    "X_recon2 = tl.tucker_to_tensor(core, latents)\n",
    "print(np.allclose(X_recon, X_recon2))\n",
    "print(np.allclose(X_recon2,X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
